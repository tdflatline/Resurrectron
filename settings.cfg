# Config file for all modules.

# Config settings for soul creation (extract.py)
[soul]
# Quote engine mode: If true, we do not tag any terms for HMM generation,
# and instead read the input corpus in to be used directly as output.
# Use of this mode is recommended your input corpus is small
# (~50 tweets or less), or if preserving exact quotes is important.
#
# Brain files based on soul files as quote engines will also not train
# an HMM, and instead use the TF-IDF query response logic directly on the
# quote pool.
quote_engine_only = False

# tweet_topics is a guess at the number of topics the target user
# tends to talk about. k-means clustering is performed to group tweets
# into these topics, and a separate HMM will be trained on each topic.
# This should help keep the HMM output on same topic while generating
# phrases.
#
# Note that a bug in nltk may cause an assert to fire if you set this
# too high. If that happens, lower this number.
# Does not apply if quote_engine_only=True
tweet_topics = 4

# Attempt AGFL usage. If false, will only use nltk.pos_tag.
# Does not apply if quote_engine_only=True
attempt_agfl = True

# If using AGFL, reject sentences that AGFL produces no tags for.
# Setting this to false may make your output noisier. However, it
# may also help you capture certain idioms and odd phrases.
# Does not apply if quote_engine_only=True
reject_agfl_failures = True

# Fall back to nltk.pos_tag when AGFL doesn't fully tag a sentence.
# If set to false, your resulting output will be better, but this
# may come at the expense of certain types of expressions. Set to
# false if you have a large body of text to work with and correctness
# is important.
# Does not apply if quote_engine_only=True
agfl_nltk_fallback = True

# If true, summarize .posts down as opposed to taking consecutive
# sentences of <= 140 chars.
post_summarize = False

# Number of sentences to use in a summarized post
post_summarize_len = 5

# Split each post into phrases of this length (summarized or not).
# This should match brain.tweet_len below.
post_len = 140

# Gzip-compress the resulting soul file.
gzip_soul = True

# Log level for soul generation
# XXX: Not yet used.
soul_log_level = "INFO"

# Config settings for brain creation (resurrect.py). These
# settings only affect tweet generation, and do not apply if
# soul.quote_engine_only is true
[brain]
# POS tag context for the state-labels of the HMM. Subjectively 4,1 seems
# to be a good balance between speed and grammar.
#
# Higher numbers will produce better grammar at the expense of taking a
# longer time to fill the tweet pool due to a higher tendnecy to generate
# tweets that are very similar to ones on the soul corpus (see
# max_shared_word_ratio below).
hmm_context = 4

# Offset into the tag context for the current term. For example:
# context=4,offset=1 gives state labels of (tag,current_tag,tag,tag)
hmm_offset = 1

# If this is non-zero, the tweet pool will have a number of messages
# equal to this multiple of the tagged soul corpus.
tweet_pool_multiplier = 1.5

# The maximum size of the tweet pool. If tweet_pool_multiplier is set,
# this value will act as a cap on that value. Otherwise, it acts as the
# total number of tweets to generate. Note that we require approx 300MB
# of memory for every 1000 tweets in the tweet pool.
tweet_pool_max = 2500

# Max length for each tweet/response (should match soul.post_len above)
tweet_len = 140

# If a generated tweet shares more than this fraction of words with any
# already generated tweet, or with any tweet in the soul corpus, it is
# rejected. Setting this too low will cause tweet pool generation to
# take a really long time.
max_shared_word_ratio = 0.75

# How many new tweets before we save the .brain file when generating tweets
save_brain_every = 100

# gzip the brain file.
gzip_brain = True

# Log level for brain generation
# XXX: Not yet used.
brain_log_level = "INFO"


# TF-IDF query parameters. These apply to both HMM-generated tweets
# and to quote_engine tweets.
[query]
# The AI has contextual memory for each person that sends it a message.
# It also remembers the most recent thing it said when it recieves an @msg.
# The way this works is via a TF-IDF score vector. This constant governs
# how fast it "forgets" this score vector. The closer to 1.0, the shorter
# the memory. Note that there is also logic that increases the decay rate
# depending on the fraction of nouns present in the query vs the previous
# statement in ConversationContext.
memory_decay_rate = 0.25

# These four weights provide multipliers to increase the signficance
# of nouns, verbs, adjectives, and adverbs. We support weighting any
# tag used by the trimmed nltk upenn tagset.
# (For tags, see POSTrim in resurrect.py and nltk.help.upenn_tagset())
# Non-specified weights are set to 1.0, except for tags specified in
# QueryStripper in resurrect,py, which are effectively 0.
NN_weight = 2.0
JJ_weight = 1.5

# If true, use the en library's wordnet module to expand our TF-IDF
# search+vocabulary to use the hyponyms, hypernyms, and antonyms of each
# word. Currently does NOT do Word-sense disambiguation, and can potentially
# artificially inflate the score for each word.
generalize_terms = True

# Min @msg reply score. Messages with a TF-IDF score less than this
# won't recieve a reply.
min_msg_reply_score = 0.333

# min follow reply score
min_follow_reply_score = 0.666

# Config settings for body
[body]
