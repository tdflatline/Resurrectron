# Config file for all modules.


# XXXXXXXXX CURRENTLY ONLY A MOCKUP. NOT YET USED XXXXXXXXX


# Config settings for soul creation (extract.py)
[soul]
# Attempt AGFL usage. If false, will only use nltk.pos_tag.
attempt_agfl = True

# If using AGFL, reject sentences that AGFL produces no tags for
reject_agfl_failures = True

# Fall back to nltk.pos_tag when AGFL doesn't fully tag a sentence.
# If set to false, your resulting output will be better, but this
# may come at the expense of certain types of expressions. Set to
# false if you have a large body of text to work with and correctness
# is important.
agfl_nltk_fallback = True

# If true, summarize .posts down as opposed to taking consecutive
# sentences of <= 140 chars.
post_summarize = False

# Gzip-compress the resulting soul file.
gzip_soul = True

# Log level for soul generation
soul_log_level = "INFO"

# Config settings for brain creation (resurrect.py)
[brain]
# Tag context for the state-labels of the HMM. 4 seems to be a good
# balance between speed and grammar. Higher numbers will produce better
# grammar at the expense of taking a longer time to fill the tweet pool
# due to a higher tendnecy to generate tweets that are very similar to
# ones on the soul corpus (see max_shared_word_ratio below). Having
# a very large corpus may help counteract this, due to larger vocab size
# for the HMM.
hmm_context = 4

# Offset into the tag context for the current term. For example:
# context=4,offset=1 gives state labels of (tag,current_tag,tag,tag)
hmm_offset = 1

# If a generated tweet shares more than this fraction of words with any
# already generated tweet, or with any tweet in the soul corpus, it is
# rejected. Setting this too low will cause tweet pool generation to
# take a really long time.
max_shared_word_ratio = 0.75

# If this is non-zero, the tweet pool size will be this multiple of
# the tagged soul corpus.
tweet_pool_multiplier = 2.0

# The maximum size of the tweet pool. If tweet_pool_multiplier is set,
# this value will act as a cap on that value. Otherwise, it acts as the
# total number of tweets to generate. Note that we require approx 300MB
# of memory for every 1000 tweets in the tweet pool.
tweet_pool_size_max = 5000

# gzip the brain file.
gzip_brain = True

# Log level for brain generation
brain_log_level = "INFO"

# Config settings for body
[body]
