Ideas for future work.

Body Tasks:
 - Give the AI a body (twitter+foursquare)
   - Post at random time intervals
     - Learn sleep cycles of input corpus if jtwt is available.
       - Do this via post/hour frequency models
       - Allow user to specify a model
   - Follow people when they follow it
   - Respond to @msgs/direct messages/high TF-IDF followed messages
   - Attempt to learn things about people the AI follows/who @msg it
     - Invert personal pronouns
     - Maintain a separate SearchableTextCollection to use TF-IDF on
       for each person that it follows/receives an @msg from.
     - Probably no HMM, just kick back the fact-based statements with
       inverted pronouns.
 - Use FourSquare top12 locations
   - use 4square data for location tags for tweets
   - The bot can also geotag tweets with 4square location information
     and actually check in to locations on the 4square list.
     - 4square has an api for this we can use.
     - we can prime the memory vector with terms related to the current
       4square location so tweets while it is "at" a location are topical.


AI/ML Tasks:
 - Consider applying POS TF-IDF weights during training
   - The POS tags for the trained set should be better, esp if we use AGFL
   - Maybe half the weight during training, half during test
 - Wordnet:
   - Consider adding tropnyms and entailment to wordnet use.
     May need to use nltk's wordnet instead of en's. Unless
     en automatically includes troponyms and entailment in hyponyms?
     - If we do this we can possibly ditch the en dependency.
   - Add config options to disable individual wordnet sets
   - May need Word-sense disambiguation to trim down wordnet's
     hyponym and hypernym sets.
 - Need to move capitalization model to bigram w/ unigram fallback...
   - some words should only captialized in pairs/streaks, others
     can be capitalized by themselves..
 - Experiment with LDA as opposed to K-Means+TF-IDF for topic clustering
   - Also is TF-IDF the right vector to use, or should it just be
     binary bag of words?
   - http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf
   - Python implementations:
     - https://mlpy.fbk.eu/
     - http://pages.cs.wisc.edu/~andrzeje/research/delta_lda.html
     - http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html
 - We can improve nltk.pos_tag using:
   https://groups.google.com/group/nltk-users/browse_thread/thread/5af97ff339e13b5a
   http://streamhacker.com/2008/11/03/part-of-speech-tagging-with-nltk-part-1/
   http://streamhacker.com/2008/11/10/part-of-speech-tagging-with-nltk-part-2/
   http://streamhacker.com/2008/12/03/part-of-speech-tagging-with-nltk-part-3/
   - We may also want to strip off the subject off of training data.
     Tweets seem to omit "I", "You", "do you", etc, and this confuses
     nltk.pos_tag as is...
   - My intuition tells me that some kind of smoothing step might be useful:
     - https://groups.google.com/group/nltk-users/msg/c52df8fafe6b0948
     - You take a second pass over the data using POS 4grams to smooth
       tags using current word + prev and next two POS tags
       - This will take less training data to train than a straight 4gram
         model
   - If we improve nltk.pos_tag enough, we can drop AGFL entirely...
     - However, we'll need to preserve certain other more specific grammar
       transitions in some other statistical model though.. maybe a bigram
       smoother?
     - Or more specific POS tags. We have enough context.
     - Or, just extract the relevant rules from AGFL by hand.
 - AGFL tweaks
   - Add modern lexicon like 'blog', 'tweet', 'gmail', 'email' to the grammar
   - Deal with AGFL's [1] and [2] POS tree references..
   - Retry questions as statements if they fail..
 - Use URL contents for TF-IDF in quote_engine_only mode.
 - Do map-reduce for faster training on multi-core or even multi-node systems
   - The expensive functions are pos_tag() in extract.py and the
     __phrase_worker2() loop in resurrect.py (the combination of
     self.voice.say_something and self.__did_already_tweet)
   - This would yield a two stage map-reduce: one stage to tag
     the corpus and the next to generate a tweet pool.
   - execnet seems like the solution we'd want:
     http://streamhacker.com/2009/11/29/distributed-nltk-execnet/
 - Other random XXX's, FIXME's and TODO's in the source.

General:
 - Create a FAQ.txt
   - Which params to tweak if query responses suck
   - Which params to tweak if language sucks
   - Which params to tweak if training takes too long
 - Create an INSTALL.txt
   - Use README.txt, and create a proper README.txt file

Hypothetical/speculation:
 - Alter HMM output based on query?
   - Instead of maintaining a tweet pool for TF-IDF, we could try to directly
     influence the HMM states based on query vocab.
     - Doing this without destroying the grammar and/or just producing
       gibberish with the query words in it seems hard though. But maybe
       some HMM wizards out there know a way.
 - PGFG?
   - Maybe we should try to learn a PCFG instead of an HMM as our grammar
     model.
     - The problem with this is we may need more structure for the grammar
       than simple POS tags. AGFL might give this to us, but that means
       falling back to nltk.pos_tag is definitely out.

