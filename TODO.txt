Ideas for future work.

Shorter term:
 - Give the AI a body (twitter)
   - Post at random time intervals
     - Learn sleep cycles of input corpus if jtwt is available.
       - Do this via post/hour frequency models
       - Allow user to specify a model
   - Respond to @msgs/direct messages
   - Use some really high TF-IDF threshold to @msg followers when they say
     something "interesting"
     - Will also need to rate limit this behavior in case multiple
       people train AIs on the same corpus. That could lead to
       conflagration...
   - Follow people when they follow it
   - Attempt to learn things about people the AI follows/who @msg it
     - Invert personal pronouns
     - Maintain a separate SearchableTextCollection to use TF-IDF on
       for each person that it follows/receives an @msg from.
     - Probably no HMM, just kick back the fact-based statements with
       inverted pronouns.
 - AGFL tweaks
   - Add modern lexicon like 'blog', 'tweet', 'gmail', 'email' to the grammar
   - Deal with AGFL's [1] and [2] POS tree references..
   - Retry questions as statements if they fail..
 - Other random XXX's, FIXME's and TODO's in the source.

Longer term:
 - randomly lament about missing Foursquare top12 locations
 - May need Word-sense disambgiuation to trim down wordnet's
   hyponym and hypernym sets.
 - train multiple HMM's on topically classified/tagged text.
   - This will prevent topic bleedover and keep tweets more sensible.
   - Can possibly even cluster corpus tweets using an unsupervised algorithm
     - But be sure to use wordnet's synsets, hyponyms, hypernyms, and
       antonyms as input to cluster alg.
     - Use tf-idf score vectors as the vectors to cluster
     - Load a .topics file from the target_user directory that summarizes
       each topic the AI will talk about, written how the target user
       would say it.
 - Do map-reduce for faster training on multi-core or even multi-node systems
   - The expensive functions are pos_tag() in extract.py and the
     __phrase_worker2() loop in resurrect.py (the combination of
     self.voice.say_something and self.__did_already_tweet)
   - This would yield a two stage map-reduce: one stage to tag
     the corpus and the next to generate a tweet pool.
 - Alter HMM output based on query?
   - Instead of maintaining a tweet pool for TF-IDF, we could try to directly
     influence the HMM states based on query vocab.
     - Doing this without destroying the grammar and/or just producing
       gibberish with the query words in it seems hard though. But maybe
       some HMM wizards out there know a way.
 - PGFG?
   - Maybe we should try to learn a PCFG instead of an HMM as our grammar
     model.
     - The problem with this is we may need more structure for the grammar
       than simple POS tags. AGFL might give this to us, but that means
       falling back to nltk.pos_tag is definitely out.

